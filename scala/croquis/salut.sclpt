/** Scala script mode */
val a = 1 + 2 + 3

def sumArray(arr: Seq[Int]) = arr.sum

object ADT {
  sealed trait TreeNode

  final case class EmptyNode() extends TreeNode

  final case class BinaryNode(
    value: Int,
    left: TreeNode,
    right: TreeNode
  ) extends TreeNode

  final case class LeafNode(
    value: Int
  ) extends TreeNode
}

import ADT._

def depth(root: TreeNode): Int = root match {
  case BinaryNode(_, left, right) =>
    Math.max(depth(left), depth(right)) + 1
  case LeafNode(_) => 1
  case EmptyNode() => 0
}

val root = BinaryNode(7,
  BinaryNode(3,
    EmptyNode(),
    LeafNode(4)
  ),
  BinaryNode(9,
    LeafNode(11),
    LeafNode(12)
  )
)

depth(root)

import scala.annotation.tailrec

def sum(f: Int => Int)(a: Int, b: Int): Int = {
  @tailrec def loop(a: Int, acc: Int): Int = {
    if (a > b) return acc
    loop(a + 1, acc + f(a))
  }
  loop(a, 0)
}

interp.load.ivy("org.scalameta" %% "scalameta" % "1.7.0")
import scala.meta._

// This does not work in Ammonite REPL
// class main extends scala.annotation.StaticAnnotation {
//   inline def apply(defn: Any) = meta {
//     val q"object $name { ..$stats }" = defn 
//     val mainDef = q"""def main(args: Array[String]): Unit = { ..$stats }"""
//     q"object $name { $mainDef }"
//   }
// }

interp.load.ivy("edu.stanford.nlp" % "stanford-corenlp" % "3.7.0")
import java.nio.file.{Path, Paths, Files}

case class FPath(fp: Path) {
  def /(fpath: FPath): FPath = new FPath(fp.resolve(fpath.fp))
}
object FPath {  
  object Implicits {
    implicit def str2fpath(s: String): FPath = new FPath(Paths.get(s))
  }
  lazy val home = new FPath(Paths.get(System.getProperty("user.home")))
}

import FPath.Implicits._
val dataRootFP = FPath.home / "local" / "data"
def loadJars(jars: Seq[String]): Unit = {
  import ammonite.ops
  jars.foreach { jarStr =>
    interp.load.cp(ops.Path((dataRootFP / jarStr).fp.toString))
  }
}
loadJars(Seq("stanford-corenlp-3.7.0-models-english.jar"))

import edu.stanford.nlp.pipeline._

val props = new java.util.Properties()
props.setProperty("annotators", "tokenize, ssplit, pos, lemma, ner, parse, dcoref");
val pipeline = new StanfordCoreNLP(props);
val annotDoc = new Annotation("able was I ere I saw elba")
pipeline.annotate(annotDoc)

import edu.stanford.nlp.ling.CoreAnnotations._
import scala.collection.JavaConverters._
val sents = annotDoc.get(classOf[SentencesAnnotation]).asScala
val toks = sents.head.get(classOf[TokensAnnotation]).asScala
toks.zipWithIndex.foreach { case (tok, idx) =>
  val word = tok.get(classOf[TextAnnotation])
  val pos = tok.get(classOf[PartOfSpeechAnnotation])
  val ent = tok.get(classOf[NamedEntityTagAnnotation])
  println(f"${idx}% 3d: ${word}%16s :: ${pos}%5s :: ${ent}%7s")
}

//interp.load.ivy("org.apache.spark" %% "spark-core" % "2.1.0")
interp.load.ivy("org.scalanlp" %% "breeze" % "0.13")
import breeze.linalg._
import breeze.optimize._

val A = CSCMatrix.zeros[Int](3, 2)
val objFn = new DiffFunction[DenseVector[Double]] {
  def calculate(x: DenseVector[Double]) = {
    (norm((x - 3d) :^ 2d, 1d), (x * 2d) - 6d)
  }
}
val lbfgs = new LBFGS[DenseVector[Double]](maxIter=100, m=3)
lbfgs.minimize(objFn, DenseVector(0, 0, 0, 0))


import org.apache.spark.sql.SparkSession
val spark = SparkSession.builder.master("local[2]").getOrCreate()
